{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel(\"C:/Users/RISHI/OneDrive/Desktop/Copper Modelling/Copper_Set.xlsx\", sheet_name=\"Result 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.columns\n",
    "numeric_conversion = ['quantity tons', 'customer', 'country','application', 'thickness', 'width','selling_price']\n",
    "df[numeric_conversion] = df[numeric_conversion].apply(pd.to_numeric, errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',187000)\n",
    "pd.set_option('display.max_columns',6000)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = df.isna().sum()\n",
    "missing_counts\n",
    "missing_percentages = (missing_counts / len(df)) * 100\n",
    "missing_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item_date\n",
    "df = df[(df['item_date'] != 19950000)]\n",
    "df = df[(df['item_date'] != 20191919)]\n",
    "#df.dropna(subset = ['item_date'] , inplace = True)\n",
    "df['item_date'] = pd.to_datetime(df['item_date'], format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delivery_Date\n",
    "#df.dropna(subset = ['delivery date'] , inplace = True)\n",
    "df = df[(df['delivery date'] != 30310101)]\n",
    "df = df[(df['delivery date'] != 20212222)]\n",
    "df['delivery date'] = pd.to_datetime(df['delivery date'], format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#material_ref\n",
    "#df[df['material_ref'].str.contains('00000000000000000000') == True].value_counts().sum()\n",
    "#df.loc[df['material_ref'].str.contains('00000000000000000000',na=False),'material_ref'] = np.NaN\n",
    "df['material_ref'] = df['material_ref'].str.lstrip('0')\n",
    "df['material_ref'] = df['material_ref'].apply(lambda x:'unknown' if pd.isna(x) else x )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantity tons\n",
    "df.loc[(df['quantity tons']=='e'),'quantity tons'] = np.NaN\n",
    "df.loc[(df['quantity tons'] <=0) , 'quantity tons'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selling_price\n",
    "df.loc[(df['selling_price'] <= 0) , 'selling_price'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country\n",
    "uc = df[df['country'].isna()]\n",
    "uc1 = uc['customer'].drop_duplicates()\n",
    "fdf = df[df['customer'].isin(uc1.tolist())]\n",
    "fdf[['customer','country']].drop_duplicates()\n",
    "\n",
    "df.loc[(df['customer'] == 30199273.00) & (df['country'].isna()),'country'] = 27.0\n",
    "df.loc[(df['customer'] == 30198586.00) & (df['country'].isna()),'country'] = 27.0\n",
    "df.loc[(df['customer'] == 30196886.00) & (df['country'].isna()),'country'] = 84.0\n",
    "#df['country'].fillna(df['country'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = df.isna().sum()\n",
    "missing_counts\n",
    "missing_percentages = (missing_counts / len(df)) * 100\n",
    "missing_percentages\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thickness\n",
    "# Apply natural log transformation to see Distribution\n",
    "df['log_thickness'] = np.log(df['thickness'])\n",
    "sns.distplot(df['log_thickness'])\n",
    "\n",
    "#Replace Missing value with Median\n",
    "#thick_median = df['thickness'].median()\n",
    "#df['thickness'].fillna(thick_median,inplace=True)\n",
    "#df[df['thickness'].isna()]\n",
    "\n",
    "#Again Apply log transformation to see Distribution\n",
    "#df['log_thickness'] = np.log(df['thickness'])\n",
    "\n",
    "#Remove Outlier based on the Lower and Upper Whisker\n",
    "# Calculate quartiles on log-transformed data\n",
    "Q1_log = df['log_thickness'].quantile(0.25)\n",
    "Q3_log = df['log_thickness'].quantile(0.75)\n",
    "\n",
    "# Calculate IQR\n",
    "IQR_log = Q3_log - Q1_log\n",
    "\n",
    "# Calculate whiskers\n",
    "lower_whisker_log = Q1_log - 1.5 * IQR_log\n",
    "upper_whisker_log = Q3_log + 1.5 * IQR_log\n",
    "\n",
    "# Find outliers in log-transformed data\n",
    "lower_outliers_log = df[df['log_thickness'] < lower_whisker_log]\n",
    "upper_outliers_log = df[df['log_thickness'] > upper_whisker_log]\n",
    "\n",
    "print(f'Q1 (Log-transformed): {Q1_log}')\n",
    "print(f'Q3 (Log-transformed): {Q3_log}')\n",
    "print(f'IQR (Log-transformed): {IQR_log}')\n",
    "print(f'Lower Whisker (Log-transformed): {lower_whisker_log}')\n",
    "print(f'Upper Whisker (Log-transformed): {upper_whisker_log}')\n",
    "\n",
    "print('\\nLower Outliers (Log-transformed):')\n",
    "print(lower_outliers_log)\n",
    "\n",
    "print('\\nUpper Outliers (Log-transformed):')\n",
    "print(upper_outliers_log)\n",
    "\n",
    "df = df[(df['log_thickness'] >= lower_whisker_log) & (df['log_thickness'] <= upper_whisker_log)]\n",
    "plt.figure(figsize=(22, 18))\n",
    "sns.distplot( df['log_thickness'])\n",
    "plt.figure(figsize=(22, 18))\n",
    "sns.boxplot( x='log_thickness',data=df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantity tons\n",
    "df['log_quantity'] = np.log(df['quantity tons'])\n",
    "plt.figure(figsize=(22, 18))\n",
    "sns.distplot( df['log_quantity'])\n",
    "df['log_quantity'].skew() #slightly negatively skewed but the outliers are acceptable\n",
    "Q1 = df['log_quantity'].quantile(0.25)\n",
    "Q3 = df['log_quantity'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_threshold_low = Q1 - 1.5 * IQR\n",
    "outlier_threshold_high = Q3 + 1.5 * IQR\n",
    "# Identify outliers\n",
    "outliers = df[(df['log_quantity'] < outlier_threshold_low) | (df['log_quantity'] > outlier_threshold_high)]\n",
    "outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selling_price\n",
    "df['log_selling_price'] = np.log(df['selling_price'])\n",
    "plt.figure(figsize=(22, 18))\n",
    "sns.boxplot( x='log_selling_price',y = 'item type',data=df)\n",
    "plt.show()\n",
    "sns.distplot(df['log_selling_price'])\n",
    "plt.show()\n",
    "Q1 = df['log_selling_price'].quantile(0.25)\n",
    "Q3 = df['log_selling_price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_threshold_low = Q1 - 1.5 * IQR\n",
    "outlier_threshold_high = Q3 + 1.5 * IQR\n",
    "# Identify outliers\n",
    "outliers = df[(df['log_selling_price'] < outlier_threshold_low) | (df['log_selling_price'] > outlier_threshold_high)]\n",
    "outliers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['width']) #skewness almost equal to Zero\n",
    "plt.show()\n",
    "sns.distplot(df['log_selling_price'])\n",
    "plt.show()\n",
    "sns.distplot(df['log_quantity'])\n",
    "plt.show()\n",
    "sns.distplot(df['log_thickness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=['width','log_selling_price','log_quantity','log_thickness']\n",
    "print(df[c].skew())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status=df['status'].unique()\n",
    "item=df['item type'].unique()\n",
    "\n",
    "print (status ,\",\" ,item)\n",
    "\n",
    "encode=LabelEncoder()\n",
    "df['status_encoded'] = encode.fit_transform(df['status'])\n",
    "\n",
    "encode=LabelEncoder()\n",
    "df['item_encoded'] = encode.fit_transform(df['item type'])\n",
    "\n",
    "status=df['status_encoded'].unique()\n",
    "item=df['item_encoded'].unique()\n",
    "print (status ,\",\" ,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "status=df['status'].unique()\n",
    "item=df['item type'].unique()\n",
    "\n",
    "encode=LabelEncoder()\n",
    "encoded_status = encode.fit_transform(status)\n",
    "\n",
    "encode=LabelEncoder()\n",
    "encoded_item_type = encode.fit_transform(item)\n",
    "\n",
    "\n",
    "with open('status.pkl', 'wb') as file:\n",
    "    pickle.dump(encoded_status, file)\n",
    "with open('item_type.pkl', 'wb') as file:\n",
    "    pickle.dump(encoded_item_type, file)\n",
    "\n",
    "with open('status.pkl', 'rb') as file:\n",
    "    encoded_status = pickle.load(file)\n",
    "with open('item_type.pkl', 'rb') as file:\n",
    "    encoded_item_type = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df[['log_quantity','status_encoded','item_encoded','application','log_thickness','width','country','customer','product_ref']]\n",
    "corr_df.corr()\n",
    "#sns.heatmap(corr_df, annot = True,cmap = \"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['log_quantity','status_encoded','item_encoded','application','log_thickness','width','country','product_ref']]\n",
    "y=df[['log_selling_price']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "#X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "with open('scaler.pkl', 'rb') as file:\n",
    "    scaled_data = pickle.load(file)    \n",
    "pred_model= scaled_data.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X), len(y)) \n",
    "print(X.shape)  # Should be (n_samples, n_features)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(pred_model,y,test_size=0.3,random_state=42)\n",
    "dtreg = DecisionTreeRegressor(random_state=42)\n",
    "dtreg.fit(x_train,y_train)\n",
    "y_predict = dtreg.predict((x_test))\n",
    "r_scr = metrics.r2_score(y_test,y_predict)\n",
    "print(r_scr)\n",
    "print(metrics.mean_squared_error(y_test,y_predict))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test,y_predict)))\n",
    "print(metrics.mean_absolute_error(y_test,y_predict)) #for outlier in the data use MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "dtreg = DecisionTreeRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=dtreg, param_grid=param_grid, \n",
    "                           cv=5, )\n",
    "grid_search.fit(x_train,y_train)\n",
    "best_dtree_reg = grid_search.best_estimator_\n",
    "y_predict = best_dtree_reg.predict((x_test))\n",
    "best_params = grid_search.best_params_\n",
    "print(metrics.mean_squared_error(y_test,y_predict))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test,y_predict)))\n",
    "print(metrics.mean_absolute_error(y_test,y_predict))\n",
    "mse = metrics.mean_squared_error(y_test, y_predict)\n",
    "rmse = mse ** 0.5\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best estimator: {best_dtree_reg}\")\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_scr = metrics.r2_score(y_test,y_predict)\n",
    "print(r_scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('regression.pkl','wb') as file:\n",
    "    pickle.dump(dtreg,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('regression.pkl','rb') as file:\n",
    "    reg = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = reg.predict([[4.02,7,3,10.00,1.79,2000.00,40.00,30225641.00,640405]])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classfier_df = df[(df['status'] == 'Won') | (df['status'] == 'Lost') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status=classfier_df['status'].unique()\n",
    "item=classfier_df['item type'].unique()\n",
    "\n",
    "print (status ,\",\" ,item)\n",
    "\n",
    "encode=LabelEncoder()\n",
    "classfier_df['status_encoded'] = encode.fit_transform(classfier_df['status'])\n",
    "\n",
    "encode=LabelEncoder()\n",
    "classfier_df['item_encoded'] = encode.fit_transform(classfier_df['item type'])\n",
    "\n",
    "status=classfier_df['status_encoded'].unique()\n",
    "item=classfier_df['item_encoded'].unique()\n",
    "print (status ,\",\" ,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = classfier_df[['log_quantity','log_selling_price','item_encoded','application','log_thickness','width','country','product_ref']]\n",
    "y = classfier_df['status_encoded']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_classify = StandardScaler().fit(X)\n",
    "X = scaler_classify.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=20)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('scaling_classify.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler_classify, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Decission_tree_classification.pkl\", 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
